\section*{Chapter 13 - Linear and Logistic Regression}

\subsection*{Solution 13.1}

I tried to calculate the equations directly from the formula, but it's significantly easier to do it with matrix differentiation.
Therefore, we develop a set of matrix differentiation tools.

For the rest of this section, let $\psi: \mathbb{R}^n \to \mathbb{R}^m$, $\bm{x} \in \mathbb{R}^m$ and $\bm{y} \in \mathbb{R}^n$, such that $\bm{y} = \bm{y}(\bm{x}) = \psi(\bm{x})$.
We define
\begin{equation*}
    \frac{\partial \psi(\bm{x})}{\partial \bm{x}}
        = \frac{\partial \bm{y}}{\partial \bm{x}}
        = \left( \frac{\partial y_i}{\partial x_j} \right)_{\substack{1 \leq i \leq m \\ 1 \leq j \leq n}}
        = \left( \begin{matrix}
            \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \ldots & \frac{\partial y_1}{\partial x_n} \\
            \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \ldots & \frac{\partial y_2}{\partial x_n} \\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \ldots & \frac{\partial y_m}{\partial x_n}
        \end{matrix} \right)
        \in \mathbb{R}^{m \times n}.
\end{equation*}


\begin{theorem} \label{md1}
If $\psi = A \in \mathbb{R}^{m \times n}$, such that $\bm{y}(\bm{x}) = A\bm{x}$, and $A$ is independent of $\bm{x}$, then $\frac{\partial \bm{y}}{\partial \bm{x}} = A$.
\end{theorem}

\begin{proof}
We have $y_i = \sum_{j = 1}^n a_{ij} x_j$, so $\frac{\partial y_i}{\partial x_j} = a_{ij}$, and therefore, $\frac{\partial \bm{y}}{\partial \bm{x}} = A$.
\end{proof}


\begin{theorem} \label{md2}
Let $\bm{y}(\bm{x}, \bm{z}) = A\bm{x}(\bm{z})$, where $\bm{x}$ is a vector values function in vector $\bm{z}$, and $A$ is independent of $\bm{x}$ and $\bm{z}$, then $\frac{\partial \bm{y}}{\partial \bm{z}} = A \frac{\partial \bm{x}}{\partial \bm{z}}$.
\end{theorem}

\begin{proof}
We have $y_i = \sum_{j = 1}^n a_{ij} x_j(\bm{z})$, so $\frac{\partial y_i}{\partial z_j} = \sum_{k = 1}^n a_{ik} \frac{\partial x_k}{\partial z_j} = \left(A \frac{\partial \bm{x}}{\partial \bm{z}}\right)_{ij}$.
Hence, $\frac{\partial \bm{y}}{\partial \bm{z}} = A \frac{\partial \bm{x}}{\partial \bm{z}}$.
\end{proof}


\begin{theorem} \label{md3}
Let $\alpha(\bm{x}, \bm{y}) = \bm{y}^t A \bm{x}$ be a real valued function, where $A$ is a matrix indepedent of $\bm{x}$ and $\bm{y}$.
We have $\frac{\partial \alpha}{\partial \bm{x}} = \bm{y}^t A$ and $\frac{\partial \alpha}{\partial \bm{y}} = (A \bm{x})^t$.
\end{theorem}

\begin{proof}
Let $\bm{z}^t = \bm{y}^t A$, so that $\alpha = \bm{z}^t \bm{x}$.
By Theorem \ref{md1} we have $\frac{\partial \alpha}{\partial \bm{x}} = \bm{z}^t = \bm{y}^t A$.
Because $\alpha$ is a scalar, $\alpha = \alpha^t = \bm{x}^t A \bm{y}$.
Applying the same method for $\bm{x}^t A$ we get $\frac{\partial \alpha}{\partial \bm{y}} = \frac{\partial \alpha^t}{\partial \bm{y}} = \bm{x}^t A^t = (A \bm{x})^t$.
\end{proof}


\begin{theorem} \label{md4}
Let $\alpha(\bm{x}) \bm{x}^t A \bm{x}$, where $A$ is independent of $\bm{x}$, then $\frac{\partial \alpha}{\partial \bm{x}} = \bm{x}^t(A + A^t)$.
\end{theorem}

\begin{proof}
We have $\alpha(\bm{x}) = \sum_{i = 1}^n \sum_{j = 1}^n x_i \alpha_{ij} x_j$, so $\frac{\partial \alpha}{\partial x_k} = \sum_{i = 1}^n x_i \alpha_{ik} + \sum_{j = 1}^n x_j \alpha_{kj} = \left(\bm{x}^t (A + A^t)\right)_{k}$.
\end{proof}

Directly from Theorem \ref{md4} we have,

\begin{theorem} \label{md5}
If $A$ is symmetic, i.e., $A^t = A$, then $\frac{\partial}{\partial \bm{x}} \bm{x}^t A \bm{x} = 2 \bm{x}^t A$.
\end{theorem}


\begin{theorem} \label{md6}
Let $\alpha(\bm{x}, \bm{y}, \bm{z}) = \bm{y}(\bm{z})^t \bm{x}(\bm{z})$, then $\frac{\partial \alpha}{\partial \bm{z}} = \bm{x}^t \frac{\partial \bm{y}}{\partial \bm{z}} + \bm{y}^t \frac{\partial \bm{x}}{\partial \bm{z}}$.
\end{theorem}

\begin{proof}
We have $\alpha(\bm{x}, \bm{y}, \bm{z}) = \sum_{i = 1}^n y_i(\bm{z}) x_i(\bm{z})$, so $\frac{\partial \alpha}{\partial z_j} = \sum_{i = 1}^n \frac{\partial y_j}{\partial z_j} x_i + \sum_{i = 1}^n y_i \frac{\partial x_i}{\partial z_j}$.
Hence $\frac{\partial \alpha}{\partial \bm{z}} = \bm{x}^t \frac{\partial \bm{y}}{\partial \bm{z}} + \bm{y}^t \frac{\partial \bm{x}}{\partial \bm{z}}$.
\end{proof}


Directly from Theorem \ref{md6} we have,

\begin{theorem} \label{md7}
    Let $\alpha(\bm{x}, \bm{z}) = \bm{x}(\bm{z})^t \bm{x}(\bm{z})$, then $\frac{\partial \alpha}{\partial \bm{z}} = 2 \bm{x}^t \frac{\partial \bm{x}}{\partial \bm{z}}$.
\end{theorem}


\begin{theorem} \label{md8}
    Let $\alpha(\bm{x}, \bm{y}, \bm{z}) = \bm{y}(\bm{z})^t A \bm{x}(\bm{z})$, where $A$ is independent of $\bm{x}$ and $\bm{y}$, then $\frac{\partial \alpha}{\partial \bm{z}} = \bm{x}(\bm{z})^t A^t \frac{\partial \bm{y}}{\bm{z}} + \bm{y}(\bm{z})^t A \frac{\partial \bm{x}}{\partial \bm{z}}$.
\end{theorem}

\begin{proof}
    Let $\bm{w}(\bm{z}) = A^t \bm{y}(\bm{z})$, such that $\alpha(\bm{x}, \bm{z}, \bm{w}) = \bm{w}(\bm{z})^t \bm{x}(\bm{z})$.
    By Theorem \ref{md2}, $\frac{\partial \bm{w}}{\partial \bm{z}} = A^t \frac{\partial \bm{y}^t}{\partial \bm{z}}$.
    By Theorem \ref{md6}, $\frac{\partial \alpha}{\partial \bm{z}} = \bm{x}(\bm{z})^t \frac{\partial \bm{w}}{\partial \bm{z}} + \bm{w}(\bm{z})^t \frac{\partial \bm{x}}{\partial \bm{z}} = \bm{x}(\bm{z})^t A^t \frac{\partial \bm{y}(\bm{z})}{\bm{z}} + \bm{y}(\bm{z})^t A \frac{\partial \bm{x}(\bm{z})}{\partial \bm{z}}$.
\end{proof}


\begin{theorem} \label{md9}
    Let $\alpha(\bm{x}, \bm{z}) = \bm{x}(\bm{z})^t A \bm{x}$, where $A$ is independent of $\bm{x}$, then $\frac{\partial \alpha}{\partial \bm{z}} = \bm{x}(\bm{z}) (A + A^t) \frac{\partial \bm{x}}{\partial \bm{z}}$.
\end{theorem}

\begin{proof}
    Directly from Theorem \ref{md8}.
\end{proof}


\begin{theorem} \label{md10}
    Let $\alpha(\bm{x}, \bm{z}) = \bm{x}(\bm{z})^t A \bm{x}$, where $A$ is independent of $\bm{x}$.
    If $A$ is symmetric, i.e., $A^t = A$, then $\frac{\partial \alpha}{\partial \bm{z}} = 2 \bm{x}^t A \frac{\partial \bm{x}}{\partial \bm{z}}$.
\end{theorem}

\begin{proof}
    Directly from Theorem \ref{md9}
\end{proof}


Now we can follow Hasty, with a lot of extra calculations.
Let
\begin{equation*}
    \mathrm{RSS}(\bm{\beta}) = (\bm{y} - X\bm{\beta})^t (\bm{y} - X\bm{\beta}).
\end{equation*}
We want to minimize $\mathrm{RSS}(\bm{\beta})$.
To do so we differentiate with respect to $\bm{\beta}$, using matrix differentiation.
\begin{equation*}
    \frac{\partial \mathrm{RSS}(\bm{\beta})}{\partial \bm{\beta}}
        = (\bm{y} - X\bm{\beta})^t(-X) + (\bm{y} - X\bm{\beta})^t(-X)
        = -2(\bm{y} - X\bm{\beta})^t X.
\end{equation*}
Solving the equation to zero gives estimate $\hat{\bm{\beta}} = (X^t X)^{-1} X^t \bm{y}$.

In the case of $n = 1$, we have
\begin{equation*}
    X = \begin{pmatrix}
        1 & X_1 \\
        1 & X_2 \\
        \ldots & \ldots \\
        1 & X_m
    \end{pmatrix}, \quad
    \bm{y} = \begin{pmatrix}
        Y_1 \\
        Y_2 \\
        \ldots \\
        Y_m
    \end{pmatrix}, \quad
    X^t \bm{y} = \begin{pmatrix}
        \sum Y_i \\
        \sum X_i Y_i
    \end{pmatrix}, \quad
    X^t X = \begin{pmatrix}
        n & \sum X_i \\
        \sum X_i & \sum X_i^2
    \end{pmatrix},
\end{equation*}
so that
\begin{equation*}
    \begin{split}
        \hat{\bm{\beta}}
            &= (X^t X)^{-1} X^t \bm{y} \\
            &=  \frac{1}{n \sum X_i^2 - \left(\sum X_i\right)^2}
                \begin{pmatrix}
                    \sum X_i^2 & -\sum X_i \\
                    -\sum X_i & n
                \end{pmatrix}
                \begin{pmatrix}
                    \sum Y_i
                    \sum X_i Y_i
                \end{pmatrix} \\
            &= \frac{1}{n \sum X_i^2 - \left(\sum X_i\right)^2}
                \begin{pmatrix}
                    \left(\sum X_i^2\right)\left(\sum Y_i\right) - \left(\sum X_i\right)\left(\sum X_i Y_i\right) \\
                    n \left(\sum X_i Y_i\right) - \left(\sum X_i\right)\left(\sum Y_i\right)
                \end{pmatrix}.
    \end{split}
\end{equation*}
In particular,
\begin{equation*}
    \begin{split}
        \hat{\beta}_1
            &= \frac{
                n \left(\sum X_i Y_i\right) - \left(\sum X_i\right)\left(\sum Y_i\right)
            }{
                n \sum X_i^2 - \left(\sum X_i\right)^2
            }
            = \frac{
                \sum X_i Y_i - \frac{1}{n}\left(\sum X_i\right)\left(\sum Y_i\right)
            }{
                \sum X_i^2 - \frac{1}{n}\left(\sum X_i\right)\left(\sum X_i\right)
            }
            = \frac{
                \sum (X_i - \overline{X}_n)(Y_i - \overline{Y}_n)
            }{
                \sum (X_i - \overline{X}_n)^2
            }, \\
        \hat{\beta}_0
            &= \frac{
                \left(\sum X_i^2\right)\left(\sum Y_i\right) - \left(\sum X_i\right)\left(\sum X_i Y_i\right)
            }{
                n \sum X_i^2 - \left(\sum X_i\right)^2
            } \\
            &= \frac{
                \left(\sum X_i^2\right)\left(\sum Y_i\right) - \left(\sum X_i\right)\left(\sum X_i Y_i\right)
                    + \frac{1}{n} (\sum X_i) (n \sum X_i Y_i - (\sum X_i) (\sum Y_i))
            }{
                n \sum X_i^2 - \left(\sum X_i\right)^2
            } - \hat{\beta}_1 \overline{X}_n \\
            &= \frac{
                \left(\sum X_i^2\right)\left(\sum Y_i\right)
                    - (\sum X_i)^2 \frac{1}{n} \sum Y_i
            }{
                n \sum X_i^2 - \left(\sum X_i\right)^2
            } - \hat{\beta}_1 \overline{X}_n \\
            &= \overline{Y}_n - \hat{\beta}_1 \overline{X}_n.
    \end{split}
\end{equation*}
The residual is defined by $\bm{\epsilon} = Y - X \bm{\beta}$ and $\hat{\bm{\epsilon}} = Y - X \hat{\bm{\beta}}$.
Note that
\begin{equation*}
    (I - X(X^tX)^{-1}X^t)X\bm{\beta} = X\bm{\beta} - X\bm{\beta} = 0, \quad
    (X\bm{\beta})^t(I - X(X^tX)^{-1}X^t) = \bm{\beta}^tX^t - \bm{\beta}^tX^t = 0.
\end{equation*}
Therefore, to find the relationship of $\hat{\bm{\epsilon}}$ with $\bm{\epsilon}$, note that
\begin{equation*}
    E(\bm{\epsilon}^t\bm{\epsilon}|X^n)
         = \frac{1}{n} \sum_{i = 1}^n E(\epsilon_i^2|X_i)
         = \frac{1}{n} \sum_{i = 1}^n (V(\epsilon_i|X_i) - E(\epsilon_i|X_i)^2)
         = \sigma^2,
\end{equation*}
and
\begin{equation*}
    \begin{split}
        \hat{\bm{\epsilon}}^t\hat{\bm{\epsilon}}
            &= (Y - X\bm{\beta})^t(Y - X\bm{\beta}) \\
            &= Y^t (I - X(X^tX)^{-1}X^t)^t (I - X(X^tX)^{-1}X^t) Y \\
            &= Y^t (I - X(X^tX)^{-1}X^t)^2 Y \\
            &= Y^t (I - 2X(X^tX)^{-1}X^t + (X(X^tX)^{-1}X^t)^2) Y \\
            &= Y^t (I - X(X^tX)^{-1}X^t) Y \\
            &= (X\bm{\beta} + \bm{\epsilon})^t (I - X(X^tX)^{-1}X^t) (X\bm{\beta} + \bm{\epsilon}) \\
            &= \bm{\epsilon}^t (I - X(X^tX)^{-1}X^t) \bm{\epsilon}.
    \end{split}
\end{equation*}
We apply a trick, used somewhere further up in the book as well, to calculate $\hat{\bm{\epsilon}}^t\bm{\epsilon}$.
Note that $\hat{\bm{\epsilon}}^t\bm{\epsilon} \in \mathbb{R}$, so $\mathrm{tr}(\hat{\bm{\epsilon}}^t\bm{\epsilon}) = \hat{\bm{\epsilon}}^t\bm{\epsilon}$ and $E(\hat{\bm{\epsilon}}^t\hat{\bm{\epsilon}}|X^n) = \hat{\bm{\epsilon}}^t\hat{\bm{\epsilon}}$.
So,
\begin{equation*}
    \begin{split}
        E(\hat{\bm{\epsilon}}^t\hat{\bm{\epsilon}}|X^n)
            &= E(\mathrm{tr}(\hat{\bm{\epsilon}}^t\hat{\bm{\epsilon}})|X^n) \\
            &= E(\mathrm{tr}(\bm{\epsilon}^t(I - X(X^tX)^{-1}X^t)\bm{\epsilon})|X^n) \\
            &= E(\mathrm{tr}(\bm{\epsilon}^t\bm{\epsilon}(I - X(X^tX)^{-1}X^t)|X^n) \\
            &= \sigma^2 E(\mathrm{tr}(I - X(X^tX)^{-1}X^t)|X^n) \\
            &= \sigma^2 (\mathrm{tr}(I_n) - \mathrm{tr}(X(X^tX)^{-1}X^t)) \\
            &= \sigma^2 (\mathrm{tr}(I_n) - \mathrm{tr}(I_k)) \\
            &= \sigma^2 (n - k),
    \end{split}
\end{equation*}
and therefore an unbias estimator of $\sigma^2$ is
\begin{equation*}
    \hat{\sigma}^2 = \frac{1}{n - k} \bm{\hat{\epsilon}}^t \bm{\hat{\epsilon}}
        = \frac{1}{n - k} \sum_{i = 1}^k \hat{\epsilon}_i^2.
\end{equation*}
In particular for this exercise, $k = 2$, which gives the final result.


\subsection*{Solution 13.2}

Note that $E(\hat{\bm{\beta}}|X) = \bm{\beta}$, so from the definition of the variance and $X\bm{\beta} + \bm{\epsilon} = Y$,
\begin{equation*}
    \begin{split}
        V(\hat{\bm{\beta}}|X)
            &= E((\hat{\bm{\beta}} - \bm{\beta})(\hat{\bm{\beta}} - \bm{\beta})^t|X^n) \\
            &= E(((X^tX)^{-1}X^tY - \bm{\beta})((X^tX)^{-1}X^tY - \bm{\beta})^t|X^n) \\
            &= E(((X^tX)^{-1}X^t(X\bm{\beta} + \bm{\epsilon}) - \bm{\beta})((X^tX)^{-1}X^t(X\bm{\beta} + \bm{\epsilon}) - \bm{\beta})^t|X^n) \\
            &= E((X^tX)^{-1}X^t \bm{\epsilon}\bm{\epsilon}^t X(X^tX)^{-1}|X^n) \\
            &= (X^tX)^{-1}X^t E(\bm{\epsilon}\bm{\epsilon}^t|X^n) X(X^tX)^{-1} \\
            &= \sigma^2 (X^tX)^{-1} \\
            &= \frac{\sigma^2}{n S_X^2} \begin{pmatrix}
                \frac{1}{n}\sum X_i^2 & -\overline{X}_n \\
                -\overline{X}_n & 1
            \end{pmatrix}.
    \end{split}
\end{equation*}


\subsection*{Solution 13.3}

With all the previous work this exercise is almost trivial.
We have
\begin{equation*}
    \hat{\beta} = (X^tX)^{-1}X^tY
        = \frac{\sum_{i = 1}^n X_i Y_i}{\sum_{i = 1}^n X_i^2}.
\end{equation*}
The standard error is given by
\begin{equation*}
    \mathrm{se}(\hat{\beta})
        = \sqrt{V(\hat{\beta}|X^n)}
        = \sqrt{\sigma^2(X^tX)^{-1}}
        = \frac{\sigma}{||X||_2}.
\end{equation*}


\subsection*{Solution 13.4}

The bias is defined by $\mathrm{bias}(\hat{\theta}) = E_{\theta}(\hat{\theta}) - \theta$, so by definition
\begin{equation*}
    \mathrm{bias}(\hat{R}_{\mathrm{tr}}(S)) = E_{R(S)}(\hat{R}_{\mathrm{tr}}(S)) - R(S).
\end{equation*}
Firstly, as $\epsilon^* \perp \epsilon_{\mathrm{tr}}$, we have $\hat{Y}_i(S) = X_i \beta(S) + \epsilon_{\mathrm{tr}} \perp X_i \beta + \epsilon^* = Y_i^*$, hence $E(\hat{Y}_i(S) Y_i^*) = E(\hat{Y}_i(S))E(Y_i^*)$.
Secondly, $E(Y_i^*) = E(X_i \beta + \epsilon^*) = X_i \beta + E(\epsilon^*) = X_i \beta + E(\epsilon) = E(X_i \beta + \epsilon) = E(Y_i)$, and similar $E((Y_i^*)^2) = E(Y_i^2)$.
Therefore,
\begin{equation*}
    \begin{split}
        E_{R(S)}(\hat{R}_{\mathrm{tr}}(S)) - R(S)
            &= \sum_{i = 1}^n \left(
                    E((\hat{Y}_i(S) - Y_i)^2) - E((\hat{Y}_i(S) - Y_i^*)^2)
                \right) \\
            &= \sum_{i = 1}^n \left(
                    E(\hat{Y}_i(S)^2) - 2E(\hat{Y}_i(S)Y_i) + E(Y_i^2) - E(\hat{Y}_i(S)^2) + 2E(\hat{Y}_i(S)Y_i^*) - E((Y_i^*)^2)
                \right) \\
            &= \sum_{i = 1}^n \left(
                    -2E(\hat{Y}_i(S) Y_i) + 2E(\hat{Y}_i(S))E(Y_i)
                \right) \\
            &= -2 \sum_{i = 1}^n \left(
                    E(\hat{Y}_i(X) Y_i) - E(\hat{Y}_i(S))E(Y_i)
                \right) \\
            &= -2 \sum_{i = 1}^n \mathrm{Cov}(\hat{Y}_i(S), Y_i).
    \end{split}
\end{equation*}
