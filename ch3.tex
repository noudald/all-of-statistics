\section*{Chapter 3 - Expectation}

\subsection*{Solution 1}

Let $X_n$ a random variable representing the amount of money after $n$ turns.
We set $X_0 = c$.
We have $E(X_n | X_{n-1}) = \frac{1}{2}(2 X_{n-1} + \frac{1}{2} X_{n-1}) = \frac{5}{4} X_{n-1}$.
Using the tower property of conditional expectation we have
\begin{equation*}
    E(X_n) = E(E(X_n | X_{n-1}))
        = \frac{5}{4} E(E(X_{n-1} | X_{n-2}))
        = ...
        = \left(\frac{5}{4}\right)^n E(X_0)
        = \left(\frac{5}{4}\right)^n c.
\end{equation*}


\subsection*{Solution 2}

\begin{itemize}
    \item[$\rightarrow$)] TODO: This is a hand-wave proof. I should look for a better proof if I have some time.
        Suppose $V(X) = 0$, then
        \begin{equation*}
            \int_{-\infty}^{\infty} (x - \mu_X)^2 dF(x) = 0.
        \end{equation*}
        But $(x - \mu_X)^2 \geq 0$ and continuous and $F$ is right continuous, so for each $x$ we must have $f(x) = 0$ or $(x - \mu_X)^2 = 0$.
        $f(x)$ cannot be zero everywhere, so there must be an $x$ such that $(x - \mu_X)^2 = 0$, which is only once at $x = \mu_X$.
        In other words $P(X = \mu_X) = f(\mu_X) = 1$ and for all $x \neq \mu_X$ we must have $f(x) = 0$.
        Take $c = \mu_X$.
    \item[$\leftarrow$)] If $P(X = c) = 1$, then $E(X) = c$ and $E(X^2) = c^2$, and $V(X) = E(X^2) - E(X)^2 = c^2 - c^2 = 0$.
\end{itemize}


\subsection*{Solution 3}

Let $X_1, X_2, ..., X_n \sim \mathrm{Uniform}(0, 1)$.
Define $Y_n = \mathrm{max}(X_1, X_2, ..., X_n)$.
\begin{equation*}
    P(Y_n < y) = \prod_{i=1}^n P(X_i < y)
        = y^n.
\end{equation*}
So $f(y) = ny^{n-1}$ and
\begin{equation*}
    E(Y_n) = \int_0^1 ny^{n} dy
        = \frac{n}{n + 1}.
\end{equation*}


\subsection*{Solution 4}

Let $X_0 = 0$.
Note that for $n > 0$, $X_n = \sum_{i = 1}^n (1 - 2B_i)$, where $B_i \sim \mathrm{Bernoulli}(p)$.
We have $E(X_n) = n - 2\sum_i E(B_i) = n(1 - 2p)$ and $V(X_n) = 4\sum_i V(B_i) = 4np(1-p)$.


\subsection*{Solution 5}

The probability density function is $f(X = i) = p^{i - 1} (1 - p) = p^i$, as $p = \frac{1}{2}$.
We have
\begin{equation*}
    E(X) = \sum_{i = 0}^{\infty} if(X = i)
        = \sum_{i = 0}^{\infty} i p^i
        = \left( \frac{1}{1 - p} \right)'
        = \frac{p}{(1 - p)^2}
        = 2.
\end{equation*}


\subsection*{Solution 6}

Write out the definition and do required book keeping.
\begin{equation*}
    \begin{split}
        E_Y(Y) &= \sum_y y P(Y = y)
            = \sum_y y P(r(X) = y)
            = \sum_y y P(X \in r^{-1}(y))
            = \sum_y \sum_{x \in r^{-1}(y)} y P(X = x) \\
            &= \sum_y \sum_{x \in r^{-1}(y)} r(x) P(X = x)
            = \sum_x r(x) P(X = x)
            = E_X(r(X)).
    \end{split}
\end{equation*}


\subsection*{Solution 7}

We first prove a lemma.
\begin{equation*}
    x P(X > x) = x \int_x^{\infty} f(t) dt
        \leq \int_x^{\infty} t f(t) dt
        = E(X) - \int_{-\infty}^{x} t f(t) dt
        \to 0,
\end{equation*}
as $x \to \infty$.
So $\lim_{x \to \infty} x(1 - F(x)) = \lim_{x \to \infty} x P(X > x) = 0$.

For the solution, use integration by parts.
\begin{equation*}
    \int_0^{\infty} P(X > x) dx = \int_0^{\infty} (1 - F(x)) dx
        = x(1 - F(x))|_0^{\infty} + \int_0^{\infty} xf(x) dx
        = E(X).
\end{equation*}


\subsection*{Solution 12}

We calculate the expected value and variance for all distributions in section 3.4.

\begin{itemize}
\item[(a)] Point mass distribution. $E(X) = \sum x p(x) = a p(a) = a$ and $V(X) = E(X^2) - E(X)^2 = a^2 - a^2 = 0$.

\item[(b)] Bernoulli, $X \sim \mathrm{Bernoulli}(p)$. $E(X) = 1p + 0(1-p) = p$.
Note that $E(X^2) = 1^2p + 0^2(1-p) = p$, so $V(X) = E(X^2) - E(X)^2 = p - p^2 = p(1 - p)$.

\item[(c)] Binomial, $X \sim \mathrm{Binomial}(n, p)$.
Write $X = \sum_{i = 1}^n X_i$, where $X_i \sim \mathrm{Bernoulli}(p)$.
Then $E(X) = E(\sum X_i) = np$ and $V(X) = V(\sum X_i) = np(1 - p)$.

\item[(d)] Geometric, $X \sim \mathrm{Geometric}(p)$.
\begin{equation*}
E(X) = \sum_{x = 1}^{\infty} xp(1 - p)^{x - 1}
    = p \left(-\sum_{x = 0}^{\infty} (1 - p)^{x}\right)'
    = p \left(-\frac{1}{p}\right)'
    = \frac{p}{p^2}
    = \frac{1}{p},
\end{equation*}
and
\begin{equation*}
E(X(X - 1)) = \sum_{x = 2}^{\infty} x(x - 1) p (1 - p)^{x - 1}
    = p (1 - p) \left(\sum_{x = 0}^{\infty} (1 - p)^{x}\right)''
    = p (1 - p) \left(\frac{1}{p}\right)''
    = \frac{2(1 - p)}{p^2}.
\end{equation*}
So $E(X^2) = E(X(X - 1)) + E(X) = \frac{2(1 - p)}{p^2} + \frac{1}{p} = \frac{2 - p}{p^2}$.
Such that $V(X^2) = E(X^2) - E(X)^2 = \frac{2 - p}{p^2} - \frac{1}{p^2} = \frac{1 - p}{p^2}$.

\item[(e)] Poisson, $X \sim \mathrm{Poisson}(\lambda)$.
\begin{equation*}
E(X) = \sum_{x = 0}^{\infty} x \frac{\lambda^x}{x!} e^{-\lambda}
    = \lambda e^{-\lambda} \sum_{x = 0} \frac{\lambda^{x-1}}{(x-1)!}
    = \lambda e^{-\lambda} e^{\lambda}
    = \lambda.
\end{equation*}
Note that
\begin{equation*}
E(X(X-1)) = \sum_{x = 0}^{\infty} x(x - 1) \frac{\lambda^x}{x!} e^{-\lambda}
    = \lambda^2 e^{-\lambda} \sum_{x = 2}^{\infty} \frac{\lambda^{x-2}}{x!}
    = \lambda^2.
\end{equation*}
So $E(X^2) - E(X)^2 = E((X - 1)X) + E(X) - E(X)^2 = \lambda^2 + \lambda - \lambda^2 = \lambda$.

\item[(f)] Uniform, $X \sim \mathrm{Uniform}(a, b)$. First take $U \sim \mathrm{Uniform}(0, 1)$, then
\begin{equation*}
\begin{split}
E(U) &= \int_0^1 xf(x)dx = \left.\frac{1}{2}x^2\right|_0^1 = \frac{1}{2}, \\
E(U^2) &= \int_0^1 x^2f(x)dx = \left.\frac{1}{3}x^3\right|_0^1 = \frac{1}{3}.
\end{split}
\end{equation*}
So
\begin{equation*}
\begin{split}
E(X) &= E((b - a)U + a) = (b - a) \frac{1}{2} + a = \frac{1}{2}(a + b), \\
V(X) &= V((b - a)U + a) = (b - a)^2 V(U) = (b - a)^2 (E(U^2) - E(U)) = \frac{1}{12} (b - a)^2.
\end{split}
\end{equation*}

\item[(g)] Normal, $X \sim \mathrm{Normal}(\mu, \sigma^2)$.
First take $Z \sim \mathrm{Normal}(0, 1)$.
Because $xf_z(x) = x\phi(x)$ is anti-symmetric $E(Z) = 0$.
To calculate $V(Z) = E(Z^2)$ we need the following lemma.

Let $\phi$ be monotonic decreasing function with finite moments on some interval $(a, \infty)$ such that $\phi(z) \to 0$ if $z \to \infty$, then $z\phi(z) \to 0$ if $z \to \infty$.
Indeed
\begin{equation*}
\int_{z/2}^{\infty} \phi(t)dt
    \geq \int_{z/2}^{z} \phi(t)dt
    \geq \int_{z/2}^{z} \phi(z)dt
    = \frac{1}{2} z\phi(z)
    > z\phi(z),
\end{equation*}
when $z$ is large enough such that $\phi(t)$ is monotonic decreasing.
As $\phi \to 0$ as $z \to \infty$, we have
\begin{equation*}
z\phi(z) < \int_{\frac{z}{2}}^{\infty} \phi(t)dt \to 0.
\end{equation*}
So $z\phi(z) \to 0$ as $z \to \infty$.

Now we calculate
\begin{equation*}
E(Z^2) = \int_{-\infty}^{\infty} z^2\phi(z) dz
    = 2\int_{0}^{\infty} z^2\phi(z) dz
    = \left[\Phi(z) - z\phi(z)\right]_{0}^{\infty}
    = 2\lim_{z \to \infty} \Phi(z) - 2\Phi(0)
    = 2 - 1
    = 1
\end{equation*}
Where we use the lemma $z\phi(z) \to 0$ if $z \to \infty$.
So $V(Z) = E(Z^2) = 1$.

Now $X = \sigma Z + \mu$, so $E(X) = \sigma E(Z) + \mu = \mu$ and $V(X) = \sigma^2 V(Z) = \sigma^2$.

\item[(h)] Exponential, $X \sim \mathrm{Exp}(\beta)$.
Using integration by parts
\begin{equation*}
\begin{split}
E(X) &= \int_0^{\infty} \frac{x}{\beta} \mathrm{exp}\left(-\frac{x}{\beta}\right) dx \\
    &= -\left.x \exp\left(-\frac{x}{\beta}\right)\right|_0^{\infty} + \frac{1}{\beta}\int_0^{\infty} \beta \exp\left(-\frac{x}{\beta}\right) dx \\
    &= \left.-\beta \exp\left(-\frac{x}{\beta}\right)\right|_0^{\infty}
    = \beta.
\end{split}
\end{equation*}
And
\begin{equation*}
E(X^2) = \int_0^{\infty} \frac{x^2}{\beta} \mathrm{exp}\left(-\frac{x}{\beta}\right) dx
    = -\left. x\exp\left(-\frac{x}{\beta}\right)\right|_0^{\infty} + 2 \int_0^{\infty} x \exp\left(-\frac{x}{\beta}\right) dx
    = 2 \beta E(X) = 2 \beta^2.
\end{equation*}
So $V(X) = E(X^2) - E(X)^2 = 2 \beta^2 - \beta^2 = \beta^2$.

\item[(i)] Gamma, $X \sim \mathrm{Gamma}(\alpha, \beta)$.
\begin{equation*}
\begin{split}
E(X) &= \int_{0}^{\infty} \frac{\beta^{\alpha} x^{\alpha}}{\Gamma(\alpha)} \exp(-\beta x) dx \\
    &= \frac{\alpha}{\beta} \int_0^{\infty} \frac{\beta^{\alpha + 1} x^{(\alpha + 1) - 1}}{\Gamma(\alpha + 1)} \exp(-\beta x) dx \\
    &= \frac{\alpha}{\beta} \int_0^{\infty} f(x; \alpha + 1, \beta) dx
    = \frac{\alpha}{\beta}.
\end{split}
\end{equation*}
And
\begin{equation*}
\begin{split}
E(X^2) &= \int_{0}^{\infty} \frac{\beta^{\alpha} x^{\alpha + 1}}{\Gamma(\alpha)} \exp(-\beta x) dx \\
    &= \frac{\alpha(\alpha + 1)}{\beta^2} \int_0^{\infty} \frac{\beta^{\alpha + 2} x^{(\alpha + 2) - 1}}{\Gamma(\alpha + 2)} \exp(-\beta x) dx \\
    &= \frac{\alpha(\alpha + 1)}{\beta^2} \int_0^{\infty} f(x; \alpha + 2, \beta) dx
    = \frac{\alpha(\alpha + 1)}{\beta^2}.
\end{split}
\end{equation*}
So $V(X) = E(X^2) - E(X)^2 = \frac{\alpha}{\beta^2}$.

\item[(j)] Beta, $X \sim \mathrm{Beta}(\alpha, \beta)$.
\begin{equation*}
\begin{split}
E(X) &= \int_0^1 \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha} (1 - x)^{\beta - 1} dx \\
    &= \frac{\alpha}{\alpha + \beta} \int_0^1 \frac{\Gamma(\alpha + 1 + \beta)}{\Gamma(\alpha + 1) \Gamma(\beta)} x^{(\alpha + 1) - 1} (1 - x)^{\beta - 1} dx
    = \frac{\alpha}{\alpha + \beta}.
\end{split}
\end{equation*}
For the variance
\begin{equation*}
\begin{split}
E(X^2) &= \int_0^1 \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha + 1} (1 - x)^{\beta - 1} dx \\
    &= \frac{\alpha (\alpha + 1)}{(\alpha + \beta)(\alpha + \beta + 1)} \int_0^1 \frac{\Gamma(\alpha + 2 + \beta)}{\Gamma(\alpha + 2) \Gamma(\beta)} x^{(\alpha + 2) - 1} (1 - x)^{\beta - 1} dx
    = \frac{\alpha(\alpha + 1)}{(\alpha + \beta)(\alpha + \beta + 1)},
\end{split}
\end{equation*}
so that
\begin{equation*}
V(X) = E(X^2) - E(X)^2
    = \frac{\alpha}{\alpha + \beta} + \frac{\alpha(\alpha + 1)}{(\alpha + \beta)(\alpha + \beta + 1)}
    = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}.
\end{equation*}

\item[(k)] Student-t, $X \sim t(\nu)$.
For $\nu > 1$, $xf(x;\nu)$ is odd, so $E(X) = 0$.
Let $\nu > 2$, variance TODO.

\item[($\ell$)] $\chi^2$ distribution, $X \sim \chi^2_p$.
We calculate all moments of $X$.
\begin{equation*}
\begin{split}
E(X^k) &= \frac{1}{2^{p/2} \Gamma(p/2)} \int_0^{\infty} x^{k + p/2 - 1} e^{-x/2} dx \\
    &= \frac{2}{2^{p/2} \Gamma(p/2)} \int_0^{\infty} (2y)^{k + p/2 - 1} e^{-y} dy \\
    &= \frac{2^{k + p/2}}{2^{p/2} \Gamma(p/2)} \int_0^{\infty} y^{k + p/2 - 1} e^{-y} dy
    = \frac{2^{k}}{\Gamma(p/2)} \Gamma\left(k + \frac{p}{2}\right).
\end{split}
\end{equation*}
So $E(X) = p$, $E(X^2) = p(p + 1)$, and $V(X) = E(X^2) - E(X)^2 = p(p + 1) - p^2 = p$.

\item[(m)] Multinomial is explained in the book.

\item[(n)] Multi-Normal, $X \sim \mathrm{Normal}(\mu, \Sigma)$.
By Theorem 2.44, if $Z \sim \mathrm{Normal}(0, I)$, then $E(Z) = 0$ and $V(Z) = I$.
By Theorem 2.43, $X = \Sigma^{\frac{1}{2}} (Z - \mu)$.
By lemma 3.21, $E(X) = E(\Sigma^{1/2}Z + \mu) = \mu$ and $V(X) = V(\Sigma^{1/2}Z + \mu) = \Sigma^{1/2} V(Z) \Sigma^{1/2} = \Sigma$.

\end{itemize}


\subsection*{Solution 15}

We have
\begin{equation*}
E(2X - 3Y) = \frac{1}{3} \int_0^2 \int_0^1 (2x - 3y)(x + y) dx dy
    = \frac{1}{3} \int_0^2 (\frac{2}{3} - \frac{1}{2}y - 3y^2) dy
    = \frac{1}{3} (\frac{4}{3} - 1 - 8)
    = - \frac{23}{9},
\end{equation*}
and
\begin{equation*}
\begin{split}
E((2X - 3Y)^2) &= \frac{1}{3} \int_0^2 \int_0^2 (2x - 3y)^2 (x + y) dx dy \\
    &= \frac{1}{3} \int_0^1 \int_0^2 (4x^3 - 8x^2y - 3xy^2 + 9y^3) dy dx \\
    &= \frac{1}{3} \int_0^1 (8x^3 - 16x^2 - 8x + 36) dx \\
    &= \frac{1}{3} (2 - \frac{16}{3} - 4 + 36)
    = \frac{86}{3}.
\end{split}
\end{equation*}
So that $V(2X - 3Y) = E((2X - 3Y)^2) - E(2X - 3Y)^2 = \frac{245}{81}$.
