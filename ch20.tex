\section*{Chapter 20 - Non-Parametric Curve Estimation}

\subsection*{Solution 20.1}

\begin{itemize}
    \item[(a)]
        We have
        \begin{equation*}
            E_{X_i}(\hat{f}(x))
                = \frac{1}{n} \sum_{i = 1}^n \frac{1}{h} \int K\left(\frac{x - y}{h}\right) f(y) dy
                = \frac{1}{n} \sum_{i = 1}^n \frac{1}{h} \int_{x - \frac{1}{2}h}^{x + \frac{1}{2}h} f(y) dy
                = \frac{1}{h} \int_{x - \frac{1}{2}h}^{x + \frac{1}{2}h} f(y) dy.
        \end{equation*}
        For calculating the variance note that $E(K(x^2)) = E(K(x))$, so
        \begin{equation*}
            \begin{split}
                V_{X_i}(\hat{f}(x))
                    &= V_{X_i}\left(\frac{1}{hn} \sum_{i = 1}^n K\left(\frac{x - X_i}{h}\right)\right) \\
                    &= \frac{1}{h^2 n^2} \sum_{i = 1}^n V\left(K\left(\frac{x - X_i}{h}\right)\right) \\
                    &= \frac{1}{h^2 n^2} \sum_{i = 1}^n \left[
                        \int_{x - \frac{1}{2}h}^{x + \frac{1}{2}h} f(y) dy
                        - \left(\int_{x - \frac{1}{2}h}^{x + \frac{1}{2}h} f(y) dy \right)^2
                    \right] \\
                    &= \frac{1}{h^2 n} \left[
                        \int_{x - \frac{1}{2}h}^{x + \frac{1}{2}h} f(y) dy
                        - \left(\int_{x - \frac{1}{2}h}^{x + \frac{1}{2}h} f(y) dy \right)^2
                    \right].
            \end{split}
        \end{equation*}
    \item[(b)] This exercise can only be true if some extra requirements are put on $f$, which are not.
        As a counter example take $f(x) = 1$ if $1 < x < 2$ or $x = 0$, and $f(x) = 0$ otherwise.
        Then $\hat{f}_n(0) = 0$, but $f(0) = 1$.
        So $P(|\hat{f}_n(0) - f(0)| < \epsilon) = 0$ for all $\epsilon < 1$, and not $\hat{f}_n(0) \xrightarrow{P} f(0)$!
\end{itemize}


\subsection*{Solution 20.2}

See code.


\subsection*{Solution 20.3}

See code.


\subsection*{Solution 20.4}

This is a classical exercise.
\begin{equation*}
    \begin{split}
        R(g, \hat{g}_n)
            &= E(L(g, \hat{g}_n)) \\
            &= E\left(\int (g(u) - \hat{g}_n(u))^2 du\right) \\
            &= \int E((g(u) - \hat{g}_n(u))^2) du \\
            &= \int E(g(u) - \hat{g}_n(u))^2 + V(g(u) - \hat{g}_n(u)) du \\
            &= \int E(g(u) - \hat{g}_n(u))^2 du + \int V(\hat{g}_n(u)) du
            = \int b^2(u) du + \int v(u) du.
    \end{split}
\end{equation*}


\subsection*{Solution 20.5}

As $x$ is fixed and $x \in B_i$, $\hat{f}(x) = \frac{\hat{p}_i}{h}$.
So $E(\hat{f}(x)) = E(\hat{p}_i/h) = p_i/h$, as $\hat{p}_i$ is the maximum likelihood estimator of $p_i$.
For the variance, note that $\hat{p}_i = \nu_i / n$.
We can write $\nu_i = \sum_{j=1}^n X_j$ where $X_j \sim \mathrm{Bernoulli}(p_i)$.
Hence $\nu_i \sim \mathrm{Binomial}(n, p_i)$.
So $V(\hat{f}(x)) = \frac{1}{n^2 h^2} V(\nu_i) = \frac{p_i(1 - p_i)}{n^2 h^2}$.


\subsection*{Solution 20.6}

We split the solution into several parts.
Note that we have
\begin{equation*}
    \hat{J}(h) = \int \hat{f}^2(x) dx - \frac{2}{n} \sum_{i = 1}^n \hat{f}_{(-i)}(x_i).
\end{equation*}
The first part can be written to
\begin{equation*}
    \begin{split}
        \int \hat{f}^2(x) dx
            &= \int \sum_{i = 1}^m \sum_{j = 1}^m \frac{\hat{p}_i \hat{p}_j}{h^2} I(x \in B_i \cap B_j) dx \\
            &= \sum_{i = 1}^m \frac{\hat{p}_i^2}{h^2} \int I(x \in B_i) dx \\
            &= \frac{1}{h} \sum_{i = 1}^m \hat{p}_i^2.
    \end{split}
\end{equation*}
This gives the first part of the solution.
